---
title: "Daily Step Count and Spotify Audio Features:"
subtitle: "An Exploration using Multiple Linear Regression & Principle Component Analysis"
author: "Spencer Zeigler"
date: "May 2nd, 2022"
output:
  pdf_document: 
    toc: true
    toc_depth: 3
    number_sections: true
    df_print: kable
editor_options:
  chunk_output_type: console # switch to 'inline' if preferred
bibliography: Stats_Spotify.bib
csl: geology.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning=FALSE)

# load libraries
library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(spotifyr) # R Wrapper for the 'Spotify' Web API
library(jsonlite) # A Simple and Robust JSON Parser and Generator for R
library(VIM) # Visualization and Imputation of Missing Values
library(naniar) # Data Structures, Summaries, and Visualisations for Missing Data
library(zoo) # S3 Infrastructure for Regular and Irregular Time Series (Z's Ordered Observations)
library(xts) # eXtensible Time Series
library(corrplot) # Visualization of a Correlation Matrix
library(car) # Companion to Applied Regression
library(performance) # Assessment of Regression Models Performance
library(olsrr) # Tools for Building OLS Regression Models
library(gtable) # Arrange 'Grobs' in Tables
library(gtsummary) # Presentation-Ready Data Summary and Analytic Result Tables
library(cols4all) # Colors for all
library(patchwork) # The Composer of Plots
library(tidymodels) # Easily Install and Load the 'Tidymodels' Packages
library(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools
library(citation)

#functions
source("scripts/functions.R")

# connect spotifyr to spotify developer app
Sys.setenv(SPOTIFY_CLIENT_ID = 'b98702e41ca940509e67f7b297d87c2e')
Sys.setenv(SPOTIFY_CLIENT_SECRET = 'a06b83088a974c3696875a7d9fb13114')

access_token <- get_spotify_access_token()

# general
theme_set(theme_bw())
colors <- c("#F3C300", "#875692", "#F38400", "#A1CAF1", "#BE0032", "#C2B280", "#848482", "#008856", "#E68FAC", "#0067A5")
colors2 <- c("#88CCEE", "#CC6677", "#DDCC77", "#117733", "#332288", "#AA4499", "#44AA99", "#999933", "#882255")

```


\newpage 

# Introduction 

It is rare that we get the opportunity to examine our own personal data. Usually, our online footprint is bought and sold so that we can be a data point in some company's targeted advertising algorithm. While my data definitely is being used to target advertising, I also pay Spotify a monthly fee so that they track my music usage and send it to me! The file that Spotify sends you is massive and contains an overwhelming amount of information, including time and location stamps on each song streamed, how long you listened to it, which device played the song (car, speaker, computer, etc.), whether or not you were on 'shuffle', and so much more. The most interesting feature of Spotify's data comes from their API, which assigns 'audio features', like danceability, energy, acousticness, tempo, and more, to every song in their database. Unfortunately, exactly how these audio features are calculated or what "danceability" might encompass is proprietary information, so this data has to be taken at face value. Despite this, Spotify's data is rich, fascinating, and lends numerical insight into something I have only been able to assess from my own, biased perspective--what do my music habits say?

Since I have had Spotify throughout the 6 years I have lived in Colorado, I was interested in examining patterns in the audio features of the music I listen to and my daily steps (tracked by FitBit)  Specifically, I set out to answer these questions: 

1. Can a specific combination of audio features explain how active I am (ie. on the number of steps I take per day)? 
- I will use *Multiple Linear Regression*

2. Are there groupings of audio features that describe the variance in my data or are there no significant groupings? 
- I will use *Principle Component Analysis*

In this paper, I will discuss how I got my data tidied and combined to answer these questions, the results of exploratory data analysis, and the models I created/methods I explored. 

# Methods 

## Obtaining and Tidying Data

I obtained two different data sets which came in two different formats, each of which required its own cleaning and missing value analysis. The first data set I acquired was from FitBit. I have been wearing a FitBit fitness tracker since ~2016 and all personal data can be requested from their website. This data comes in a zip file which contains hundreds of JSON files. Thankfully, there is a package called `jsonlite` which reads JSON files into R as dataframes, which I used in combination with `purrr::map`, to read in all of these files at once. The raw FitBit data stores your steps data in one JSON per month, and records the number of steps you've taken per ~1 minute increments. To be in a useful format, I had to convert the days into a format readable by R and then summarize to get the total number of steps per day. As for missing values, there were 4 total days that have 0 steps. There are more days with an extremely low number of steps (<20) which indicates I left it to charge for 95% of the day and just put it on before bed. I only counted 0 step days as missing, and there is no systematic pattern to these missing days. I decided to impute these NA values with the median value.

```{r read and tidy steps data, include = FALSE}
# These files take forever to read/load in, so all this code was run once and the output was saved to an RDS object which is then loaded in at the end. 

# steps_raw <- dir("data/", pattern = "steps-*") |>
#   map_df(~fromJSON(file.path("data/", .), flatten = TRUE))
#   
# steps <- steps_raw |>
#   separate(dateTime, into = c("date", "time"), sep = " ")
# 
# steps <- steps |>
#   mutate(date = lubridate::mdy(date), 
#          time = lubridate::hms(time), 
#          steps = as.numeric(value)) |>
#   select(-c(value)) |>
#   filter(date >= "2016-03-01")
# 
# steps <- steps |> 
#   group_by(date) |> 
#   summarize(steps_daily = sum(steps))
#   
# steps$steps_daily[steps$steps_daily == 0] <- NA

# apply(steps, 2, VIM::countNA)
# which(is.na(steps$steps_daily), arr.ind = TRUE)

#There are 4 total days that have 0 steps. There are more days with a silly number of steps (<20) which indicates I left it to charge for 95% of the day and just put it on before bed. I will only count 0 step days as NA. There is no systematic pattern to these missing days. I think the best thing to do is to impute values with the median, so as to exclude the effect of outliers. 

# steps <- naniar::impute_median_all(steps)

steps <- readRDS("cache/steps.rds")
```

The 'extended listening history' dataset I acquired took ~3 weeks to arrive after requesting it from Spotify's privacy team. The data arrived as a large zip file containing JSON files with multitudes of information-- but I focused on the streaming history which was a set of 9 JSON files. Cleaning this data was focused mostly on acquiring the audio features for each song I streamed, which is not standard information included in the extended listening history. The `spotifyr` package is a wrapper for pulling audio feature information from Spotify's API. Unfortunately, this interface with the API only allows yu to collect this data for 20 songs at a time, so I wrote a function to use the `get_track_audio_features()` function to collect and organize the audio features for all ~128,000 songs I have streamed. Detailed information about the audio features can be found in S1. Most of the audio features are measured on a 0-1 scale, but I normalized the few that did not (eg. loudness, measured in decibels) so I could more easily compare them. I want to note that there are 2,294 instances of streamed songs don't have track name/artist name/Spotify info/or album name. This is probably a result of music I uploaded to my personal Spotify library since it wasn't available on this platform. Since these songs are missing at random, I felt it is best to remove these entries since they are a small portion of my overall data set (1.75%) and I can't impute values into them. 

```{r read and tidy spotify data, include = FALSE}
# read in all .json files from directory as dataframes using jsonlite
spotify_raw <- dir("data/", pattern = "endsong_\\d.json") |>
  map_df(~fromJSON(file.path("data/", .), flatten = TRUE))
  
# examine NA
apply(spotify_raw, 2, VIM::countNA)
hist(which(is.na(spotify_raw$master_metadata_track_name), arr.ind=TRUE), main = "Which row numbers contain NA?",  xlab= "Row number") 
#I see that the NA are randomly scattered
  
spotify <- spotify_raw |> 
  select(ts, ms_played, conn_country, master_metadata_track_name, master_metadata_album_artist_name, master_metadata_album_album_name, spotify_track_uri, reason_start, reason_end, shuffle) |> 
  drop_na() |> 
  separate(ts, into = c("date", "time"), sep = "T") |> 
  mutate(time = str_replace(time, "Z", "")) |> 
  mutate(date = lubridate::as_date(date), 
         time = lubridate::hms(time)) |>  #convert time to proper format
  separate(spotify_track_uri, into = c("rm1", "rm2", "track_id"), sep = ":") |> 
  select(-c("rm1", "rm2"))  |> 
  filter(date >= "2016-03-01")
```

```{r get audio features, include = FALSE}
#This code was run only once to retrieve audio features. Output was saved as an RDS object in "cache/"

# track_id <- bind_cols(track_id = spotify$track_id,
#                       fl = gl(1285, k = 100)[1:128407]) |> #create factor levels to use group_split on
#   group_split(fl, .keep = FALSE)
# 
# audio_features <- list()
# 
# for (i in 1:length(track_id)) {
#   audio_features[[i]] <- spotifyr::get_track_audio_features(unlist(track_id[[i]]))
# }

#saveRDS(audio_features, "cache/audio_features.rds")
audio_features <- readRDS("cache/audio_features.rds")

audio_features <- data.table::rbindlist(audio_features) |>
  rename(track_id = id)

spotify <- bind_cols(spotify, audio_features) |>
  select(-c("track_id...24")) |>
  rename(track_id = "track_id...8")

#key, loudness, tempo, time signature are NOT 0 to 1 

spotify <- spotify |>
  mutate(loudness_norm = (loudness - min(loudness))/(max(loudness)- min(loudness)), 
         tempo_norm = (tempo - min(tempo))/(max(tempo)- min(tempo)), 
         time_signature_norm = (time_signature - min(time_signature))/(max(time_signature) - min(time_signature))) |>
  select(-c(time, conn_country, reason_start, reason_end, type, uri, track_href, analysis_url, loudness, tempo, time_signature, speechiness)) |>
  filter(master_metadata_album_artist_name != "Mark Williams") |> #Meditation track
  relocate(17:19, .before = 16)

#I am going to remove 'key', I don't think it makes sense to keep it in, even normalized.
```

The final steps in data cleaning involved combining these separate data sets into one, cohesive data set which had missing values cleaned up, a consistent data structure, and entries aligned by date properly. This final data set is called `daily_data`. This data set includes FitBit and Spotify data from 2016-05-26 to 2022-03-17 (the day I requested my Spotify extended history). When combining these data sets, I introduced some missing values (eg. on days where I had steps but listened to no music) but I decided to keep these values as `NA` because they encode important information about my habits. 

```{r daily, include = FALSE}
spotify_daily <- spotify |>
  group_by(date) |>
  summarize(mins_daily = sum(ms_played)/60000, 
            danceability_daily = mean(danceability), 
            energy_daily = mean(energy),
            loudness_daily = mean(loudness_norm), 
            acousticness_daily = mean(acousticness),
            instrumentalness_daily = mean(instrumentalness),
            liveness_daily = mean(liveness),
            valence_daily = mean(valence), 
            tempo_daily = mean(tempo_norm))

daily_data <- full_join(spotify_daily, steps, join_by = "date") #introduce NA here, days where i have steps but no music data 
daily_data <- filter(daily_data, date >= "2016-05-26" & date <= "2022-03-14") #Chose to filter from this date since I have extremely limited music data before this date

apply(daily_data, 2, VIM::countNA)
which(is.na(daily_data$mins_daily), arr.ind = TRUE)
#All NA are at the bottom of the dataset but the dates they span are random ie. MAR.

#I did not remove NA's from this data set. They're probably interesting...
```

Before starting to explore questions about patterns and relationships between music and steps, I scaled my data using the `scale()` function, $Z = \frac{x - \bar{x}}{\sigma}$, and split my `daily_data` set into an 80/20 training/testing set. I did this to avoid 'double dipping' and so I could use the MSPE to evaluate model fitness.  

```{r scale, include = FALSE}
daily_data_scaled <- scale(daily_data[,2:11]) |> 
  bind_cols(date = daily_data$date) |>
  relocate(date, .before = 1)
```

```{r train test, include = FALSE}
set.seed(123)
train_daily <- sample_n(daily_data_scaled, 0.8*nrow(daily_data))
test_daily <- anti_join(daily_data_scaled, train_daily)
```

```{r remove, include = FALSE}
rm(spotify_raw)
rm(spotify_daily)
rm(steps)
rm(audio_features)
```

## Exploratory Data Analysis 

The first goal with my completed data set was to understand the range of the scaled values and to examine my assumptions about the statistical distributions of my variables of interest. For my first question, I am primarily interested in the variables corresponding to audio features (Fig. 1) and to `steps_daily` (Fig. 2). I need to assess if my data can be used in a linear regression model, and more specifically, in a least squares linear regression model. I have to assess: 

1. Linearity
2. Independence
3. Homoscedasticity 
4. Normality

```{r table_summary, fig.caption = "Table 1. Summary table showing the median and the 25th, 75th percentile and the number of missing values."}
gtsummary::tbl_summary(train_daily) |> 
  modify_caption("Table 1. Summary table showing the median and the 25th, 75th percentile and the number of missing values.")
```

The summary table shown in Table 1, as expected because we standardized the data, shows the mean of all variables very close to 0. This summary table also notes that the audio features have 49 NA values each-- these NA values are left in this data set since they represent days that I took steps but did not listen to music. Conversely, the `steps_daily` variable has 22 NA values which represent days that I listened to music but did not take any steps. 

```{r audiofeatures, fig.cap = "Histograms of each audio feature.", fig.height=4, fig.width=5}
train_daily |>
  select(2:11) |> 
  pivot_longer(1:9) |> 
  ggplot(aes(x = value)) +
  geom_histogram(fill = colors[[2]], color = "black", size = 0.2, bins = 15) +
  facet_wrap(~name, scales = "free", nrow = 3)  +
  labs(x = "Standardized Audio Feature Score", 
       y = "Count") +
  theme(panel.grid = element_blank(), 
        legend.position = "none") 
# God DAMN my tempo is almost perfectly normally distributed...
```

```{r steps, fig.cap="Histogram of my response, steps_daily.", fig.height=3, fig.width=5}
train_daily |> 
  select(steps_daily) |> 
  #pivot_longer() |> 
  ggplot(aes(x=steps_daily)) +
  geom_histogram(fill = colors[[2]], color = "black", size = 0.2,bins = 15) +
  #facet_wrap(~name, scales = "free") +
  labs(x = "Standardized Steps Per Days", 
       y = "Count") +
  theme(panel.grid = element_blank(), 
        legend.position = "none") 
```

A few of the audio features histograms look normally distributed (eg. tempo, danceability) but none of the others do. The normality assumption for linear regression states that $Y_i \sim N(\beta_0 + \beta_1x_{i,1} + .. + \beta_px_{i,p}, \sigma^2)$ and looking at the `steps_daily` plot, we can see that it appears to be approximately normal. Thankfully, the normality assumption for linear regression is the least important and therefore can be relaxed. Using a log transformation on the data does not result in a more obviously normal distribution. To assess the possibility of outliers, we can look at the boxplots in Fig. 3, where there are clearly many outliers. For acousticness and instrumentalness, the outliers are on the high end but for energy and loudness the outliers tend to be on the low end. This makes sense, since my music taste, on average, tends to be on the higher end of the energy/loudness scale and lower on the acousticness/instrumentalness scale, each with a very limited range, so outliers are easy to concentrate on the opposite half. The other audio features have more evenly spread outliers on either side. These outliers are not 'incorrect' in any way and therefore will not be removed. 

```{r boxplot, fig.cap = "Boxplot of the audio features. Note outliers.", fig.height=4, fig.width=5}
train_daily |>
  select(1, 3:10) |> 
  pivot_longer(2:9) |> 
  ggplot(aes(x = name, y= value)) +
  geom_boxplot(color = colors[[8]], size = 0.3) +
  labs(x = "Audio Feature", 
       y = "Standardized Audio Feature Score") +
  theme(axis.text.x = element_text(angle = 30, vjust = 0.9, hjust=1), 
        plot.margin = margin(1,1,0.5,1, "cm"), 
        axis.title.x.bottom = element_text(vjust = -1), 
        panel.grid = element_blank())
```

```{r table_cor, fig.cap = "Table 2. Correlation matrix. Note that large correlations indicate collinearity will be an issue for this dataset."}
#library(xtable)
mcor <- cor(na.omit(train_daily[,2:10]))
upper<- round(mcor,2)
upper[upper.tri(mcor)]<-""
upper<-as.data.frame(upper)
names(upper) <- NULL
rownames(upper) <- NULL


headers <- c("mins", "danceability", "energy", "loudness ","acousticness", "instrumentalness", "liveness", "valence", "tempo")

names(upper) <- headers
rownames(upper) <- headers

knitr::kable(upper, 
             caption = "Table 2. Correlation matrix. Note that large correlations indicate collinearity will be an issue for this dataset.")
```


The correlation matrix in Table 2 shows that we have some potential issues with collinearity, which is obvious from the underlying theory. Loudness/energy and acousticness/instrumentalness show large positive correlations within each pair and large negative correlation between pairs. Other audio features show moderate correlations (~0.6), but I predict that the collinearity issue is mainly caused by the loudness/energy and acousticness/instrumentalness dichotomy. Since it is clear that collinearity will be an issue in this data set, I will first fit a standard linear regression and perform model selection. Then, I will attempt some shrinkage methods to reduce the variance in my data set and come up with a set of non-correlated predictors via Principle Component Analysis. 

# Multiple Linear Regression 

## Model Selection

To answer my first question, *can a specific combination of audio features explain how active I am*, I fit a multiple linear regression on my training data using `steps_daily` as the response and all audio features as the predictors: 

`lm(steps_daily ~ mins_daily + danceability_daily + energy_daily + loudness_daily + acousticness_daily + instrumentalness_daily + liveness_daily + valence_daily + tempo_daily)`

```{r table_lmod, fig.cap = "Table 3. Summary output of full model."}
train_daily <- train_daily |>
  select(2:11) 

lmod_steps <- lm(steps_daily ~ ., train_daily)

knitr::kable(broom::tidy(lmod_steps), 
             caption = "Table 3. Summary output of full model.")
```

Looking at the summary table for the regression output (Table 3), I first notice that I have significant predictors (`mins_daily`, `danceability_daily`, `tempo_daily`, `instrumentalness_daily`)! This indicates to me that there is some linear relationship between audio features and the number of steps I take per day. However, the intercept is NOT significant, meaning that we failed to reject $H_0 = 0$ for $H_1 \neq 0$. Additionally, the full f-test for this regression is significant at the $\alpha < 0.05$ level which tells us that the intercept only model is not sufficient (ie. I need at least some of these predictors in my model). 

### Selecting AIC Based Model
 
After fitting this initial model, I completed two distinct model selection procedures-- bidirectional stepwise selection using AIC as the criterion (`olsrr::ols_step_both_aic`) and bidirectional stepwise selection using BIC as the criterion (`leaps::resubsets`). Then, I compare these models using the Mean Squared Prediction Error (MSPE) on my test set. 

The Akaike Information Criterion (AIC) is a better metric for assessing your model if the goal is prediction-- my goal is not prediction, but since AIC might pick a larger model than BIC and these are both criterion-based model selection tools, I wanted to try it. AIC balanced between the model fit (residual sum of squares) and model complexity (number of predictors). AIC has a smaller penalty for adding predictors: 

$$AIC = 2(p +1) + n log(\frac{RSS}{n})$$

```{r table_aic_stepwise, fig.cap = "Table 4. A summary table for the model selected by AIC."}
model_selection <- ols_step_both_aic(lmod_steps)
#model_selection <- ols_step_all_possible(lmod_steps)
#both ols_step_all_possible and ols_step_both_aic select the same model

#plot(model_selection$aic)

lmod_steps_aic <- lm(steps_daily ~ mins_daily + danceability_daily + instrumentalness_daily + tempo_daily, data = train_daily)

#summary(lmod_steps_aic)
knitr::kable(broom::tidy(lmod_steps_aic), 
             caption = "Table 4. A summary table for the model selected by AIC.")
```

```{r, fig.height=9, fig.width =7, fig.cap = "Model peformance plots for model fit with AIC."}
performance::check_model(lmod_steps_aic)
```

```{r fitted v observed aic, fig.cap="Fitted vs. observed plot for model fit with AIC. Data should fall along 1:1 line if the model is specified correctly.", fig.height=3, fig.width=5}
fitted_obs_aic <- as.data.frame(cbind(fitted = fitted(lmod_steps_aic), 
                                  obs = na.omit(train_daily)$steps_daily))

ggplot(fitted_obs_aic, aes(x= fitted, y = obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  labs(x = "Fitted", 
       y= "Observed")
# plot(fitted(lmod_steps_aic), na.omit(train_daily)$steps_daily)
# abline(1,1)
```

The model selected by stepwise AIC has `mins_daily`, `danceability`, `instrumentalness`, and `tempo` as predictors for the number of steps I take per day. The summary table (Fig. 4) once again shows an insignificant intercept, but significant predictors, and a significant full f-test. I am surprised by the sign of the tempo estimate, which implies that for each unit increase in the number of steps I take per day, the tempo of my music goes down. The model performance plot for this model (Fig. 4) shows that none of the regression assumptions are violated-- linearity, homoscedasticity, independence, and normality of residuals all look good! Despite the residuals vs. fitted plot looking flat with random scatter around the y = 0 line, the graph used to assess the linearity assumption shown in Fig. 5 shows a very non-linear trend. We would expect to see the points falling along the 1:1 here if the model specified is correct but we do not see this; instead we see a large collection of points right in the middle of the plot. There is not another pattern (eg. groupings, a parabola) that is obvious besides a lack of linearity. This indicates that while this model doesn't outright violate any of our assumptions, its explanatory power will be decreased, the estimators will not be unbiased, and the inferences of the estimators based on the misspecified regression model will be biased and misleading. Essentially, this model should be taken with a large grain of salt. 

I also fit this model using bidirectional step-wise selection based on the p-value and the best model was the same as chosen by AIC (S2). 

### Selecting BIC Based Model

Finally, I decided to fit this model using the `regsubsets()` function from the `leaps` package as we had done in class. This method selected the model with `mins_daily`, `instrumentalness` and `tempo` as the predictors in the model that had the lowest Bayesian Information Criterion (BIC). BIC is generally better for explanatory models and tends to favor smaller models due to a larger penalty for more complex models. BIC balances the number of predictors (model complexity) and model fit (RSS):

$$BIC(g(\mathbf{x}; \mathbf{\hat{\beta}})) = (p+1)log(n)-2logL(\hat{\beta}) $$

```{r regsubsets daily, "Table 5. A summary table for the model selected by BIC."}
regs <- leaps::regsubsets(steps_daily ~ ., data = train_daily)
regs <- summary(regs)

#plot(regs$bic)

lmod_steps_bic <- lm(steps_daily ~ tempo_daily + mins_daily + instrumentalness_daily, data = train_daily)
#summary(lmod_steps_bic)
knitr::kable(broom::tidy(lmod_steps_bic), 
             caption = "A summary table for the model selected by BIC.")
```

```{r modelperfom_bic, fig.height=9, fig.width =7, fig.cap = "Model peformance plots for model fit with BIC."}
performance::check_model(lmod_steps_bic)
```

```{r fittedobs_bic, fig.cap="Fitted vs. observed plot for model fit with BIC. Data should fall along 1:1 line if the model is specified correctly.", fig.height=3, fig.width=5}
fitted_obs_bic <- as.data.frame(cbind(fitted = fitted(lmod_steps_bic), 
                                  obs= na.omit(train_daily)$steps_daily))

ggplot(fitted_obs_bic, aes(x= fitted, y = obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  labs(x = "Fitted", 
       y= "Observed")
```

Similarly to the AIC model, this model passes all visual checks of regression assumptions (Fig. 6) but the fitted vs. observed plot is again not clearly linear (Fig. 7). 

### Model Comparison

I have two different models of different sizes, chosen by different functions and criteria. I would like to compare these models by Mean Squared Prediction Error (MSPE). MSPE is a model selection method that uses the testing set to calculate the expected difference between what my model predicts for a specific value and what the true value is: 

$$MSPE = \frac{1}{k} \sum_{i=1}^k (y_i^* - \mathbf{x_i^*\hat{\beta}})^2$$ 

Where $y_i^*$ is the $i{th}$ response value in the test set; $x_i^* = (1, x^*_{i,1}, x^*_{i,2},...,x^*_{i,p})$ is the $i{th}$ set of predictors in the in the test set; and $\hat{\beta}$ is the least squares estimate of $\beta$ fit on the training set. 

```{r model comparison}
#Mean Squared Prediction Error
mspe_aic <- mean((test_daily$steps_daily - predict.lm(lmod_steps_aic, test_daily))^2, na.rm = TRUE)
mspe_bic <- mean((test_daily$steps_daily - predict.lm(lmod_steps_bic, test_daily))^2, na.rm = TRUE)

knitr::kable(tibble(mspe_aic, mspe_bic), 
             caption = "MSPE for both MLR models.")

mod_comp <- performance::compare_performance(lmod_steps_aic, lmod_steps_bic)
knitr::kable(mod_comp, 
             caption = "Overall metrics for model comparison.")
```

We see that the model selected by AIC has a very slightly smaller MSPE than the model selected by BIC (Table 6). As shown in Table 7, the AIC is significantly smaller for the AIC selected model and the BIC are only very slightly different. It is clear than the model selected by AIC is superior according to the MSPE, AIC, and adjusted $R^2$. Therefore, my final model is: 
`steps_daily ~ mins_daily + danceability_daily + instrumentalness_daily + tempo_daily`

## Model Interpretation 

For a visual companion to the summary table below, see Fig. 8 for added-variable plots which plot each response-predictor pair while holding the other predictors constant.
Below, I will assess each model parameter in detail (Table 5). I will deal with the fact that the values that went into this model were standardized-- which impacts the interpretation of the parameters. 

```{r added_var, echo = FALSE, fig.cap = "Added variable plots for best model.", fig.width=5, fig.height=4}
avPlots.invis <- function(MODEL, ...) {
  ff <- tempfile()
  png(filename = ff)
  OUT <- car::avPlots(MODEL, ...)
  dev.off()
  unlink(ff)
  OUT }

avp <- avPlots.invis(lmod_steps_aic)

p1 <- ggplot(data = as.data.frame(avp[[1]]), 
             mapping = aes(x = mins_daily, y = steps_daily)) + 
  geom_point(fill = "transparent", color = "grey20") +
  geom_smooth(method = 'lm', formula = 'y ~x', color = "#C2B280") +
  labs(x = "Total Minutes Per Day", 
       y = "Steps Per Day") +
  theme_bw() +
  theme(panel.grid = element_blank())


p2 <- ggplot(data = as.data.frame(avp[[2]]), 
             mapping = aes(x = danceability_daily, y = steps_daily)) + 
  geom_point(color = "grey20") +
  geom_smooth(method = 'lm', formula = 'y ~x', color = "#848482") +
  labs(x = "Average Danceability Score", 
       y = "Steps Per Day") +
  theme_bw() +
  theme(panel.grid = element_blank())

p3 <- ggplot(data = as.data.frame(avp[[3]]), 
             mapping = aes(x = instrumentalness_daily, y = steps_daily)) + 
  geom_point( color = "grey20") +
  geom_smooth(method = 'lm', formula = 'y ~x', color = "#008856") +
  labs(x = "Average Instrumentalness Score", 
       y = "Steps Per Day") +
  theme_bw() +
  theme(panel.grid = element_blank())

p4 <- ggplot(data = as.data.frame(avp[[4]]), 
             mapping = aes(x = tempo_daily, y = steps_daily)) + 
  geom_point( color = "grey20") +
  geom_smooth(method = 'lm', formula = 'y ~x', color = "#E68FAC") +
  labs(x = "Average Tempo", 
       y = "Steps Per Day") +
  theme_bw() +
  theme(panel.grid = element_blank())


(p1 + p2) / (p3 + p4) + patchwork::plot_annotation(
  title = "Added-Variable Plots",
  subtitle = "steps ~ minutes + danceability + instrumentalness + tempo",
  caption = "For each plot, all other predictors are being held constant")
```

* The full F-test tests the hypothesis:
$H_0$: $Y_i = \beta_0 + \varepsilon_i$ is sufficient
$H_1$: $Y_i = \beta_0 + \varepsilon_i$ is NOT sufficient (or, some other, larger model is sufficient)

Since the f-statistic for this regression is significant at the $\alpha < 0.05$ level, we reject $H_0$ and state that we have evidence to support that the larger model is sufficient. 

Coefficients: 

* **Intercep**t:  the t-test p-value is not significant at the $\alpha < 0.05$ level which means we failed to reject the null hypothesis that $\beta_0 = 0$. But, since the parameters are scaled, I wouldn't expect the intercept to be significantly different from 0. With scaled parameters, the intercept can be interpreted as: *the average number of steps I take per day when all the the predictors are zero (ie. at their average value)*. Theoretically, I would expect the average number of steps I take to occur on days where I listen to my most 'average' music.  
  * Unscaled average number of steps: 6874.39
    * 1. The `unscaled()` function can be found in S3.


* **mins_daily**: I take 0.11 more (standardized) steps, on average, if the number of minutes I listen to music increases by 1 standard deviation, assuming all other predictors are held constant. 
  * This predictor has a significant t-test at the $\alpha < 0.05$ level, which means we can reject the null hypothesis and state that $\hat{\beta_1} \neq 0$, which means that there is a significant, non-zero, linear relationship. 
  * Unscaled number of steps: 417.903

* **danceability_daily**: I take 0.07 more (standardized) steps, on average, if the danceability score of the music I listen to increases by 1 standard deviation, assuming all other predictors are held constant. 
  * This predictor has a significant t-test at the $\alpha < 0.05$ level, which means we can reject the null hypothesis and state that $\hat{\beta_1} \neq 0$, which means that there is a significant, non-zero, linear relationship. 
  * Unscaled increase in number of steps: 290.011
  
* **instrumentalness_daily**: I take 0.12 less (standardized) steps, on average, if the instrumentalness score of the music I listen to increases by 1 standard deviation, assuming all other predictors are held constant. 
  * This predictor has a significant t-test at the $\alpha < 0.05$ level, which means we can reject the null hypothesis and state that $\hat{\beta_1} \neq 0$, which means that there is a significant, non-zero, linear relationship. 
    * Unscaled decrease in number of steps: 439.475
    
* **tempo_daily**: I take 0.09 less (standardized) steps, on average, if the instrumentalness score of the music I listen to increases by 1 standard deviation, assuming all other predictors are held constant. 
  * This predictor has a significant t-test at the $\alpha < 0.05$ level, which means we can reject the null hypothesis and state that $\hat{\beta_1} \neq 0$, which means that there is a significant, non-zero, linear relationship. 
    * Unscaled decrease in number of steps: 313.236

The results of this model are generally not surprising, which is a good thing! The more music I listen to and the more danceable it is seems to make sense as positive predictors of step count. I expected to see a decrease in the number of steps I took per day with an increase in instrumentalness; I can attribute this to the fact that periods of writing are usually accompanied by movie soundtracks and classical. 

The only predictor I am surprised by is tempo-- I would have expected an increase in the tempo of the music I listen to correspond to an increase in my activity based on theory [@karageorghis_revisiting_2011]. Instead, I may be able to attribute this to the fact that I listen to very high tempo music when I am cooking (metal) and when I am studying (electronic)-- neither of these activities are very high step, which might explain the decrease in average number of steps associated with higher tempo music. 


However, this model does not have strong explanatory power, nor are the estimates particularly meaningful due to the non-linearity between the fitted values and the observed values as seen in fig. 7. So, I would like note the difference between statistical and practical significance for this model in particular. The changes associated with a 1 standard deviation increase or decrease in the audio features tend to be on the order of hundreds of steps (1/4 of a mile = ~500 steps). Given the violation of linearity, I am hesitant to depend on this model's explanatory power for relating a 400 step decrease to an increase in the  instrumentalness of my music that day. However, since this is my personal data, I am able to reflect on these patterns in a unique way, which could be a positive or a negative, since I could find trends based on my own presuppositions. But, even admitting this, I do see some legitimate explanation and interesting patterns in this data, as I explained in the previous paragraph. This model is interesting to me on a personal, explanatory way, but I would not trust predictions from this model and nor would I try to generalize this approach. 

# Principle Component Analysis

Shrinkage methods are used to 'shrink' the number of predictors in a data set if that number is large and multicollinearity presents a problem [@lever_principal_2017]. Shrinkage methods allow you to reduce the dimensions of your data without sacrificing patterns or trends in your data which might be obscured or hidden by traditional methods of dealing with high dimensional data sets, like removing predictors [@lever_principal_2017]. I have nine predictors in my data set which the MLR models reduced to 4 and 3 predictors-- but I might be interested in how all the predictors interact and relate to each other and therefore to my step count. To explore potential clusters between audio features, I will provide a brief theoretical exploration of Principle Component Analysis and then apply and interpret the results. 

## Theory

Typically, principle component analysis is used to reduce the dimensionality of your data set by creating linear combinations of multiple variables, such that the most variance is explained by the first linear combination. This allows many variables to be condensed to only a handful of meaningful axes (ie. linear combinations that explain most of the variance). To begin making these linear combinations, we first must center and scale the data and store it in a matrix with no missing values. The first principle component is a linear combination of the variables $X_1, X_2, ..., X_p$, [@hefin_rhys_principal_2017; @holland_principal_2021]: 

$Y_1 = a_1^T\mathbf{X}$

The point of creating this linear combination is that the first principle component is designated such that it explains the greatest possible variance in your data [@holland_principal_2021]. The second principle component is calculated in the same way, but is constrained by the fact that it must be perpendicular (ie. uncorrelated) to PC1 [@holland_principal_2021; @statquest_with_josh_starmer_statquest_2018]. This continues until you have the same number of PC's as the original number of variables. The wonderful part of principle component analysis is that it does not exclude or remove any data-- it only creates linear combinations of them such that the princple components cannot be correlated, so there is no issue of collinearity [@hefin_rhys_principal_2017]. The rotation of your data can be written as [@holland_principal_2021]: 

$$Y = XA$$

The matrix $A$ is composed of rows which are eigenvectors. Each entry within a row (eg. $a_{ij}$) are the weights, or loadings, and they describe how much each individual contributes to a principle components; the higher the loading value for a variable, the stronger the relationship of that variable to the principle component while the sign dictates whether that variable and the principle component are positively or negatively correlated [@holland_principal_2021, @hartmann_choose_2018]. 

The eigenvalues are the values in the diagonal matrix $S_Y = AS_XA^T$ [@holland_principal_2021]. The eigenvalues describe the amount of variance explained by each principle component [@holland_principal_2021]]. I examine these values by looking at a skree plot. This plot allows us to accomplish the goal of using PCA-- dimension reduction [@hefin_rhys_principal_2017]. The amount of variance explained by each principle component decreases from PC1, since PC1 is initially chosen on the condition that it explains the greatest possible variance; some PC's may explain so little variance they can be ignored [@statquest_with_josh_starmer_statquest_2018]! Therefore, there are some rules of thumb for choosing the number of principle components you need [@hartmann_choose_2018]:
1) Choose the number of components you need to reach some threshold of cumulative variance (eg. 75%)
2) Choose the number of components that have a proportional variance explained greater than 1.  
3) On a skree plot, choose the number of components that occur before the "elbow" (ie. choose the PC's that explain the most variance and ignore PC's that all explain roughly the same amount)

Knowing some theoretical background to the output R will give me, I will attempt to use PCA to reduce the dimensions of my data and understand what exactly the principle components tell me about patterns within my audio features. 

## Application and Interpretation 

To do Principle Component Analysis, I will be using the `tidymodels` framework and their `step_pca()` function which allows for tidier results but relies on the functionality of `prcomp()` at the back end [@silge_understanding_2018, @silge_pca_2020].

```{r pca setup, include = FALSE}
train_daily_pc <- train_daily |> 
  select(-steps_daily) |> 
  na.omit()

pca_rec <- recipe(~., data = train_daily_pc) |> #no outcome formula 
  step_pca(all_predictors()) #do pca

pca_prep <- prep(pca_rec) #actually run the recipe

pca_tidy_coef <- recipes::tidy(pca_prep, type = "coef", 1) #variable loadings
pca_tidy_var <- recipes::tidy(pca_prep, type = "variance", 1) #variance

```

```{r pca_output}
knitr::kable(head(pca_tidy_coef), caption = "Table 8a. Main outputs of the tidymodels PCA analysis, contains the variable loadings (b) contains the variances.")

knitr::kable(head(pca_tidy_var), caption = "Table 8b. Main outputs of the tidymodels PCA analysis, contains the variances.")
```

The tables shown in Table 8a,b are the two main outputs I will be working with. `pca_tidy_coef` contains the variable loading for each component, as described in the theory section. `pca_tidy_var` contains the variance, cumulative variance, percent variance, and cumulative percent variance explain by each component [@recipes_2022]. 

```{r skree plot, fig.cap = "Skree plot showing princple component vs. percent variance explained; note that only the first 3 components are above the percent variance = 1 line in black.", fig.height=3, fig.width=5}
pca_tidy_var |> 
  filter(terms == "percent variance") |> 
  mutate(component = fct_inorder(as.character(component))) |> 
  ggplot(aes(x = component, y = value)) + 
  geom_col(aes(fill = component)) +
  scale_fill_manual(values = colors) +
  geom_hline(yintercept = 10, size = 0.4) +
  theme_bw() +
  theme(legend.position = "none", 
        panel.grid = element_blank()) +
  scale_y_continuous(limits = c(0,100)) +
  labs(x = "Principle Component", 
       y = "Percent Variance")

```

I first want to assess how many components control the most variance in my data; I will do this by looking at Fig. 9. The black line in Fig. 9 is at ~10% variance, which is a general rule of thumb for determining which components you should keep. Clearly, the majority of the variance in my data is explained by PR1, but I will include PR2 and PR3 in my analysis as well, since they explain a total of ~78% of the variance in my data and are the three components above 10% variance. 

```{r loadings, fig.cap = "Variable loadings for each predictor. The larger the bar, the more influence (weight) that predictor has on that PC."}
pca_tidy_coef |>
  filter(component %in% paste0("PC", 1:3)) |>
  mutate(component = fct_inorder(component)) |>
  ggplot(aes(value, terms, fill = terms)) +
  scale_fill_manual(values = colors) +
  geom_col(show.legend = FALSE) +
  geom_vline(xintercept = 0, linetype = 1, size = 0.4) +
  facet_wrap(~component, nrow = 1) +
  labs(y = NULL, 
       x = "Variable Loadings") +
  theme_bw() +
  theme(panel.grid = element_blank())
```

In fig. 10, I am displaying my three principle components with the loading displayed for each term. From this graph, I want to attempt to describe the clusters or patterns I can pick out.

The most obvious pattern in PC1 is that loudness/energy and instrumentalness/acousticness are opposites-- songs with high loudness/energy will have low instrumentalness/acousticness, which makes sense! Songs with high loudness/energy tend to have larger valence scores also (ie. are "happier"), which also makes sense. 

PC2 is dominated by the apparent trend that songs that have high valence and danceability scores do not also have high tempos and liveness scores. This relationship make sense when I think about my own music-- one of the only live albums I listen to has a very high tempo/energy, since this band is known for its particularly wild live shows, but the not do make 'happy' or 'danceable' music. I would be interested in attaching band names to this analysis to see if I could extract this sort of information, but this is a future project! 

PC3 is suggesting that mins_listened does not group well with any audio feature, which makes sense since it is not an audio feature! Since it is measuring a totally different metric, it makes sense that it would make its own principle component. 

```{r loadings_pc1pc2, fig.cap = "A. A zoomed in look at the percent explained variance by each term for PC1 and PC2. B. A zoomed out look at the explained variance for all my data; note that all terms are centered in the data cloud."}
pc1 <- pca_tidy_coef |> 
  filter(component %in% c("PC1", "PC2")) |> 
  pivot_wider(names_from = component, values_from = value) |> 
ggplot(aes(x=PC1, y=PC2, label = terms)) +
  geom_text(hjust = 0.3, vjust = 1.3, size = 3) +
  geom_point(aes(color = terms), show.legend = F, size = 3) +
  scale_fill_manual(values = colors) +
  xlab(paste("PC1 - ", "53.1", "%", sep="")) +
  ylab(paste("PC2 - ", "14.7", "%", sep="")) +
  scale_x_continuous(expand = expansion(0.5)) +
  scale_y_continuous(expand = expansion(0.1)) +
  theme_bw() 

pc2 <- pca_tidy_coef |> 
  filter(component %in% c("PC1", "PC2")) |> 
  pivot_wider(names_from = component, values_from = value) |> 
ggplot(aes(x=PC1, y=PC2)) +
  geom_point(juice(pca_prep), mapping = aes(PC1, PC2), alpha = 0.5, size = 2, color = "grey70") +
   geom_point(aes(color = terms), show.legend = F, size = 2.5) +
  scale_fill_manual(values = colors) +
  xlab(paste("PC1 - ", "53.1", "%", sep="")) +
  ylab(paste("PC2 - ", "14.7", "%", sep="")) +
  #scale_x_continuous(expand = expansion(0.5)) +
  theme_bw() 

pc1 + pc2 + patchwork::plot_annotation(tag_level = 'A')
  
```

To visualize how the loading scores map onto our predictors on a 2D plane, I will focus on PC1 and PC2, since they cover ~68% of the variance in my data set. Looking at plot A in fig. 11 we can see ~3 groupings (if we squint!); this is really just another way to visualize what we were seeing in fig. 10. But, to confirm: there is separation along PC1 around 0, which are the groups we previously identified-- acousticness/instrumentalness vs. loudness/energy. There is additionally some separation visible along the PC2 axis between the danceability/valence group and the tempo/liveness grouping. More importantly, we should note that despite the appearance of these groups in fig 101, we see in fig. 11b they are very clustered in the center of all the data. This indicates that there is only one "group", we cannot strongly separate out groups or factors with this analysis. However, I might anticipate that many people's Spotify data looks like this since Spotify's algorithm tends to try to shove people into a single musical identity so it can optimize its recommendation algorithm. It is much more difficult to recommend music, and make playlists, if your music taste is all over the map! 

#### Future Work 

I think it would be really fun and interesting to get the genre information for each artist (genre information isn't available on a per song basis) and then try PCA to determine which audio features dominate which genres. 

I would have done it here but the amount of time it would take to get an API running in R to extract that information and then to clean it would be ungodly, so, it'll be a future project!! 


# Conclusions 

In this project, I have detailed how to clean and tidy messy time series data and get it into a format easy to work with for both multiple linear regression and principle component analysis. I set out to explore this data to answer two main questions: 

1. Can a specific combination of audio features explain how active I am (ie. on the number of steps I take per day)? 

2. Are there groupings of audio features that describe the variance in my data or are there no significant groupings? 


The first question used an AIC and a BIC derived multiple linear regression model and used the MSPE to choose the best model. The best model did result in a specific combination of audio features which, unfortunately, had a low explanatory power due to a non-linear trend between the fitted values and the observed values. The model still revealed some interesting insights, like instrumentalness having a negative relationship with steps taken due to fact that when I listen to instrumental music I tend to be writing. 

The second question introduced me to principle component analysis. It revealed some really interesting small scale features but when we place those features in the broader context, it is clear that my audio features exist in one big 'blob'. There are no obvious groupings within my audio features, but I interpreted this to mean Spotify's algorithm is working! 

Overall, I might not have come to any big, bold, conclusions about the relationship between my steps and audio features, I got apply regression interpretations and learn about shrinkage reduction methods. These are such powerful tools and I will continue down some of the paths I started down during exploratory data analysis, but ultimately had to abandon for time constraints. This is the data set that won't stop giving and I have the tools to receive its information!

# Refrences

<div id="refs"></div>

[@tidy_2019]

Zaharatos, B. 2022, *Module4-annotated* [Lecture Slides]. STAT 5010. Univerity of Colorado Boulder.

Zaharatos, B. 2022, *Module1-1-annotated* [Lecture Slides]. STAT 5010. Univerity of Colorado Boulder.

Zaharatos, B. 2022, *Module1-2-annotated* [Lecture Slides]. STAT 5010. Univerity of Colorado Boulder.

Zaharatos, B. 2022, *Module3-annotated* [Lecture Slides]. STAT 5010. Univerity of Colorado Boulder.


# Supplementary Information

#### S1. Audio Features


acousticness-- 0 to 1; 1 is high confidence track is acoustic

danceability-- 0 to 1; 0 is least danceable, 1 is most danceable

energy-- 0 to 1; 1 is high energy

instrumentalness-- 0 to 1; values above 0.5 represent instrumental tracks, the closer the score to 1 the higher confidence

loudness-- in decibels. loudness ranges between -60 and 0 dB. 0 db is loud, -60 is quiet

mode-- major is 1; minor is 0

tempo = bpm

time signature-- number 3-7 indicating 3/4, 4,4, etc

liveness-- 0 to 1; probability track was played lived. value > 0.8 high prob of live

valence-- 0 to 1; 1= happy, 0 = sad

https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features

#### S2. 

```{r mlr stepwise p, echo = TRUE}
lmod_stepwise <- ols_step_both_p(lmod_steps) #chooses a model by p-value
lmod_steps_stepwise <- lm(steps_daily ~ mins_daily + danceability_daily + instrumentalness_daily + tempo_daily, data = train_daily)
#plot(lmod_stepwise$aic)
summary(lmod_steps_stepwise)
```

#### S3. 

```{r unscale, echo = TRUE}
unscale <- function(estimate, predictor_name) {
  predictor_name <- predictor_name
  estimate * sd(predictor_name, na.rm = TRUE) + mean(predictor_name, na.rm = TRUE)
}

unscale(-0.001453, daily_data$steps_daily)
```

#### S4. 

```{r loadings2, fig.width = 7, fig.height = 4}
pca_tidy_coef  |> 
  filter(component %in% paste0("PC", 1:3)) |>
  group_by(component) |>
  top_n(8, abs(value)) |>
  ungroup() |>
  mutate(terms = reorder_within(terms, abs(value), component)) |>
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y", nrow = 1) +
  theme_bw() +
  scale_y_reordered() +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive?"
  )
# juice(pca_prep) |>
#   ggplot(aes(PC1, PC2)) +
#   geom_point(alpha = 0.7, size = 2) +
#   # geom_text(check_overlap = TRUE, hjust = "inward", family = "IBMPlexSans") +
#   labs(color = NULL)

```










